<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>Spider | DAY's Blog</title><meta name="keywords" content="Notes,Language,Spider"><meta name="author" content="santidad DAY"><meta name="copyright" content="santidad DAY"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="网络爬虫，也叫网络蜘蛛，是一种用来自动浏览万维网的网络机器人。其目的一般为编纂网络索引，网络搜索引擎等站点通过爬虫软件更新自身的网站内容或其对其他网站的索引。网络爬虫可以将自己所访问的页面保存下来，以便搜索引擎事后生成索引供用户搜索。爬虫访问网站的过程会消耗目标系统资源。">
<meta property="og:type" content="article">
<meta property="og:title" content="Spider">
<meta property="og:url" content="https://santidadday.github.io/2023/02/12/%E7%88%AC%E8%99%AB/index.html">
<meta property="og:site_name" content="DAY&#39;s Blog">
<meta property="og:description" content="网络爬虫，也叫网络蜘蛛，是一种用来自动浏览万维网的网络机器人。其目的一般为编纂网络索引，网络搜索引擎等站点通过爬虫软件更新自身的网站内容或其对其他网站的索引。网络爬虫可以将自己所访问的页面保存下来，以便搜索引擎事后生成索引供用户搜索。爬虫访问网站的过程会消耗目标系统资源。">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://pic4.zhimg.com/v2-0b51b0f4b44bbdb55fbae70d14b299bf_180x120.jpg">
<meta property="article:published_time" content="2023-02-11T16:00:00.000Z">
<meta property="article:modified_time" content="2023-03-14T13:23:54.000Z">
<meta property="article:author" content="santidad DAY">
<meta property="article:tag" content="Notes">
<meta property="article:tag" content="Language">
<meta property="article:tag" content="Spider">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic4.zhimg.com/v2-0b51b0f4b44bbdb55fbae70d14b299bf_180x120.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://santidadday.github.io/2023/02/12/%E7%88%AC%E8%99%AB/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery@2/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Spider',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-03-14 21:23:54'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/day.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data is-center"><div class="data-item"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">17</div></a></div><div class="data-item"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">9</div></a></div><div class="data-item"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">1</div></a></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="not-top-img" id="page-header"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">DAY's Blog</a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav></header><main class="layout" id="content-inner"><div id="post"><div id="post-info"><h1 class="post-title">Spider</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="fa-fw post-meta-icon far fa-calendar-alt"></i><span class="post-meta-label">Created</span><time datetime="2023-02-11T16:00:00.000Z" title="Created 2023-02-12 00:00:00">2023-02-12</time></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Spider"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div><article class="post-content" id="article-container"><h2 id="html页面结构介绍"><a href="#html页面结构介绍" class="headerlink" title="html页面结构介绍"></a>html页面结构介绍</h2><figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;!DOCTYPE <span class="keyword">html</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">html</span> <span class="attr">lang</span>=<span class="string">&quot;en&quot;</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">head</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">meta</span> <span class="attr">charset</span>=<span class="string">&quot;UTF-8&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">title</span>&gt;</span>Title<span class="tag">&lt;/<span class="name">title</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">head</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- table 表格</span></span><br><span class="line"><span class="comment">     tr 行</span></span><br><span class="line"><span class="comment">     td 列</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">table</span> <span class="attr">width</span>=<span class="string">&quot;200px&quot;</span> <span class="attr">height</span>=<span class="string">&quot;200px&quot;</span> <span class="attr">border</span>=<span class="string">&quot;1px&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">tr</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">td</span>&gt;</span><span class="tag">&lt;/<span class="name">td</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">tr</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">table</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- ul li 无序列表</span></span><br><span class="line"><span class="comment">     ol li 有序列表</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">ul</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">ul</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">ol</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">li</span>&gt;</span><span class="tag">&lt;/<span class="name">li</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">ol</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- a 超链接</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">a</span> <span class="attr">href</span>=<span class="string">&quot;http://www.xxx.com/&quot;</span>&gt;</span><span class="tag">&lt;/<span class="name">a</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">body</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">html</span>&gt;</span></span><br></pre></td></tr></table></figure>

<hr>
<h2 id="爬虫"><a href="#爬虫" class="headerlink" title="爬虫"></a>爬虫</h2><p>如果我们把互联网比作一张大的蜘蛛网，那一台计算机上的数据便是蜘蛛网上的一个猎物，而爬虫程序就是一只小蜘蛛，沿着蜘蛛网抓取自己想要的数据</p>
<ul>
<li>通过一个程序，根据url进行爬取网页，获取有用信息</li>
<li>使用程序模拟浏览器，去向服务器发送请求，获取响应信息</li>
</ul>
<h3 id="爬虫核心"><a href="#爬虫核心" class="headerlink" title="爬虫核心"></a>爬虫核心</h3><p>爬取网页：爬取整个网页，包含网页的所有内容</p>
<p>解析数据：将网页中你所得到的数据进行解析</p>
<p>难点：爬虫和反爬虫之间的博弈</p>
<h3 id="爬虫用途"><a href="#爬虫用途" class="headerlink" title="爬虫用途"></a>爬虫用途</h3><p>数据分析&#x2F;人工数据集</p>
<p>社交软件冷启动</p>
<p>舆情监控</p>
<p>竞争对手监控</p>
<h3 id="爬虫分类"><a href="#爬虫分类" class="headerlink" title="爬虫分类"></a>爬虫分类</h3><p><strong>通用爬虫：</strong></p>
<ul>
<li>实例：百度、360、google等搜索引擎</li>
<li>功能：访问页面、抓取数据、数据存储、数据处理、提供检索服务</li>
<li>robots协议：<ul>
<li>一个约定俗成的协议，添加robots.txt文件，来说明本网站哪些内容不可以被抓取，起不到限制作用</li>
<li>自己写的爬虫无需遵守</li>
</ul>
</li>
<li>网络排名：<ul>
<li>根据pagerank算法进行排名（参考各网站流量、点击率等指标）</li>
<li>百度竞价排名</li>
</ul>
</li>
<li>确定：<ul>
<li>抓取的数据多数无用</li>
<li>不能根据用户需求来精准获取数据</li>
</ul>
</li>
</ul>
<p><strong>聚焦爬虫：</strong></p>
<ul>
<li>功能：根据需求实现爬虫，抓取需要的数据</li>
<li>设计思路：<ul>
<li>确定要爬取的url</li>
<li>模拟浏览器通过http协议访问url，获取服务器返回的html代码</li>
<li>解析html字符串（根据一定规则提取需要的数据）</li>
</ul>
</li>
</ul>
<h3 id="反爬手段"><a href="#反爬手段" class="headerlink" title="反爬手段"></a>反爬手段</h3><ul>
<li><p>User-Agent：中文名为用户代理，简称UA，它是一个特殊字符串头，使得服务器能够识别客户使用的操作系统及版本、CPU类型、浏览器及版本、浏览器渲染引擎、浏览器语言、浏览器插件等</p>
</li>
<li><p>代理IP：西次代理、快代理</p>
</li>
<li><p>什么是高匿名、匿名和透明代理？它们有什么区别？</p>
<ul>
<li>使用透明代理，对方服务器可以知道你使用了代理，并且也知道你的真实IP</li>
<li>使用匿名代理，对方服务器可以知道你使用了代理，但不知道你的真实IP</li>
<li>使用高匿名代理，对方服务器不知道你使用了代理，更不知道你的真实IP</li>
</ul>
</li>
<li><p>验证码访问</p>
</li>
<li><p>动态加载网页：网页返回的是js数据，并不是网页的真是数据</p>
</li>
<li><p>数据加密</p>
</li>
</ul>
<hr>
<h2 id="urllib"><a href="#urllib" class="headerlink" title="urllib"></a>urllib</h2><h3 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用urllib获取百度首页的源码</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个url 访问地址</span></span><br><span class="line">url = <span class="string">&#x27;http://www.baidu.com&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟浏览器向服务器发送请求 response响应</span></span><br><span class="line">response = urllib.request.urlopen(url)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取响应中页面的源码</span></span><br><span class="line"><span class="comment"># read方法返回的是字节形式的二进制数据</span></span><br><span class="line"><span class="comment"># 将二进制数据转换为字符串</span></span><br><span class="line"><span class="comment"># 二进制-&gt;字符串 (解码) decode(&#x27;编码格式&#x27;)</span></span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印数据</span></span><br><span class="line"><span class="built_in">print</span>(content)</span><br></pre></td></tr></table></figure>

<h3 id="常用类型和方法"><a href="#常用类型和方法" class="headerlink" title="常用类型和方法"></a>常用类型和方法</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">response = urllib.request.urlopen(url)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(response))</span><br><span class="line"><span class="comment"># response为HTTPResponse类型</span></span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 按照一个字节一个字节的读,返回5个字节</span></span><br><span class="line">content1 = response.read(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按照一行一行读</span></span><br><span class="line">content2 = response.readline()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按照一行一行读直到读完</span></span><br><span class="line">content3 = response.readlines()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取状态码,如果是200证明没有问题</span></span><br><span class="line">code = response.getcode()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取url地址</span></span><br><span class="line">url = response.geturl()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取状态信息(响应头)</span></span><br><span class="line">headers = response.getheaders()</span><br></pre></td></tr></table></figure>

<h3 id="下载到本地"><a href="#下载到本地" class="headerlink" title="下载到本地"></a>下载到本地</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 下载网页urlretrieve(url, filename)传入下载路径及文件名</span></span><br><span class="line">urllib.request.urlretrieve(url=url, filename=<span class="string">&#x27;baidu.html&#x27;</span>)</span><br><span class="line"><span class="comment"># 下载图片</span></span><br><span class="line">url_img = <span class="string">&#x27;图片地址&#x27;</span></span><br><span class="line">urllib.request.urlretrieve(url=url_img, filename=<span class="string">&#x27;xx.jpg&#x27;</span>)</span><br><span class="line"><span class="comment"># 下载视频</span></span><br><span class="line">url_video = <span class="string">&#x27;视频地址&#x27;</span></span><br><span class="line">urllib.request.urlretrieve(url=url_video, filename=<span class="string">&#x27;xx.mp4&#x27;</span>)</span><br></pre></td></tr></table></figure>

<h3 id="请求对象定制"><a href="#请求对象定制" class="headerlink" title="请求对象定制"></a>请求对象定制</h3><p>url组成<code>https://www.baidu.com/s?wd=xxx</code></p>
<ul>
<li>协议：http&#x2F;https</li>
<li>主机：<a target="_blank" rel="noopener" href="http://www.baidu.com/">www.baidu.com</a></li>
<li>端口号：http80、https443、mysql3306、redis6379、oracle1521、mongodb27017等</li>
<li>路径：s</li>
<li>参数：wd&#x3D;xxx</li>
<li>锚点：#</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># User-Agent：中文名为用户代理，简称UA，它是一个特殊字符串头，使得服务器能够识别客户使用的操作系统及版本、CPU类型、浏览器及版本、浏览器渲染引擎、浏览器语言、浏览器插件等</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://www.baidu.com&#x27;</span></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 因为urlopen中不能存储字典,所以headers需要转为字典类型</span></span><br><span class="line"><span class="comment"># 请求对象的定制</span></span><br><span class="line">request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟浏览器向服务器发送请求</span></span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(content)</span><br></pre></td></tr></table></figure>

<h3 id="编解码"><a href="#编解码" class="headerlink" title="编解码"></a>编解码</h3><h4 id="GRT请求方式：urllib-parse-quote"><a href="#GRT请求方式：urllib-parse-quote" class="headerlink" title="GRT请求方式：urllib.parse.quote()"></a>GRT请求方式：urllib.parse.quote()</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request, urllib.parse</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://www.baidu.com/s?wd=&#x27;</span></span><br><span class="line"><span class="comment"># 使用quote方法将汉字编程unicode编码</span></span><br><span class="line">name = urllib.parse.quote(<span class="string">&#x27;周杰伦&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 请求资源路径</span></span><br><span class="line">url = url + name</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 请求对象定制</span></span><br><span class="line">request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟浏览器向服务器发送请求</span></span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(content)</span><br></pre></td></tr></table></figure>

<h4 id="GET请求方式：urllib-parse-urlencode"><a href="#GET请求方式：urllib-parse-urlencode" class="headerlink" title="GET请求方式：urllib.parse.urlencode()"></a>GET请求方式：urllib.parse.urlencode()</h4><p>在实际开发过程中遇到较长url，使用quote非常麻烦</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request, urllib.parse</span><br><span class="line"></span><br><span class="line"><span class="comment"># https://www.baidu.com/s?wd=周杰伦&amp;sex=男&amp;location=中国台湾省</span></span><br><span class="line">url = <span class="string">&#x27;https://www.baidu.com/s?&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># urlencode应用</span></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;wd&#x27;</span>: <span class="string">&#x27;周杰伦&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;sex&#x27;</span>: <span class="string">&#x27;男&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;location&#x27;</span>: <span class="string">&#x27;中国台湾省&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">new_data = urllib.parse.urlencode(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 请求资源路径</span></span><br><span class="line">url = url + new_data</span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36&#x27;</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 请求对象定制</span></span><br><span class="line">request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟浏览器向服务器发送请求</span></span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(content)</span><br></pre></td></tr></table></figure>

<h4 id="POST请求方式"><a href="#POST请求方式" class="headerlink" title="POST请求方式"></a>POST请求方式</h4><p>post请求方式的参数必须进行编码，<code>data = urllib.parse.urlencode(data)</code></p>
<p>编码之后必须调用encode方法，<code>data = urllib.parse.urlencode(data).encode(&#39;utf-8&#39;)</code></p>
<p>参数是放在请求对象定制的方法中，<code>request = urllib.request.Request(url=url, data=data, headers=headers)</code></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request, urllib.parse</span><br><span class="line"></span><br><span class="line"><span class="comment"># post请求</span></span><br><span class="line">url = <span class="string">&#x27;https://fanyi.baidu.com/sug&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟浏览器向服务器发送请求</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;kw&#x27;</span>: <span class="string">&#x27;spider&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># post请求的参数必须进行编码,其必须为为bytes类型</span></span><br><span class="line">data = urllib.parse.urlencode(data).encode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># post请求的参数不会拼接在url后,而是需要放在请求对象定制的参数中</span></span><br><span class="line">request = urllib.request.Request(url=url, data=data, headers=headers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模拟浏览器想服务器发送请求</span></span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(content)</span><br></pre></td></tr></table></figure>

<h3 id="ajax"><a href="#ajax" class="headerlink" title="ajax"></a>ajax</h3><h4 id="GET请求"><a href="#GET请求" class="headerlink" title="GET请求"></a>GET请求</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 下载豆瓣电影动作片排行榜中第一页数据</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://movie.douban.com/j/chart/top_list?type=5&amp;interval_id=100%3A90&amp;action=&amp;start=0&amp;limit=20&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 请求对象定制</span></span><br><span class="line">request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取响应数据</span></span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据下载到本地</span></span><br><span class="line"><span class="comment"># open默认情况使用GBK编码,如果我们需要在open方法中指定编码格式为utf-8,encoding=&#x27;utf-8&#x27;</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;douban.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    fp.write(content)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 下载豆瓣电影动作片排行榜中前十页数据</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_request</span>(<span class="params">page</span>):</span><br><span class="line">    base_url = <span class="string">&#x27;https://movie.douban.com/j/chart/top_list?type=5&amp;interval_id=100%3A90&amp;action=&amp;&#x27;</span></span><br><span class="line"></span><br><span class="line">    data = &#123;</span><br><span class="line">        <span class="string">&#x27;start&#x27;</span>: (page - <span class="number">1</span>) * <span class="number">20</span>,</span><br><span class="line">        <span class="string">&#x27;limit&#x27;</span>: <span class="number">20</span></span><br><span class="line">    &#125;</span><br><span class="line">    data = urllib.parse.urlencode(data)</span><br><span class="line">    url = base_url + data</span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 请求对象定制</span></span><br><span class="line">    request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line">    <span class="keyword">return</span> request</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_content</span>(<span class="params">request</span>):</span><br><span class="line">    response = urllib.request.urlopen(request)</span><br><span class="line">    content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> content</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download</span>(<span class="params">content, page</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;douban&#x27;</span> + <span class="built_in">str</span>(page) + <span class="string">&#x27;.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        fp.write(content)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    start_page = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&#x27;请输入起始的页码:&#x27;</span>))</span><br><span class="line">    end_page = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&#x27;请输入结束的页码:&#x27;</span>))</span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(start_page, end_page + <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 每一页都有请求对象的定制</span></span><br><span class="line">        request = create_request(page)</span><br><span class="line">        <span class="comment"># 获取响应数据</span></span><br><span class="line">        content = get_content(request)</span><br><span class="line">        <span class="comment"># 数据下载到本地</span></span><br><span class="line">        download(content, page)</span><br></pre></td></tr></table></figure>

<h4 id="POST请求"><a href="#POST请求" class="headerlink" title="POST请求"></a>POST请求</h4><p>在headers中出现<code>X-Requested-With:XMLHttpRequest</code>说明该请求为ajax请求</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 下载kfc官网中北京地区的门店前10页数据</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">create_request</span>(<span class="params">page</span>):</span><br><span class="line">    url = <span class="string">&#x27;https://www.kfc.com.cn/kfccda/ashx/GetStoreList.ashx?op=cname&#x27;</span></span><br><span class="line"></span><br><span class="line">    data = &#123;</span><br><span class="line">        <span class="string">&#x27;cname&#x27;</span>: <span class="string">&#x27;北京&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;pid&#x27;</span>: <span class="string">&#x27;&#x27;</span>,</span><br><span class="line">        <span class="string">&#x27;pageIndex&#x27;</span>: page,</span><br><span class="line">        <span class="string">&#x27;pageSize&#x27;</span>: <span class="number">10</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># post请求必须进行编码</span></span><br><span class="line">    data = urllib.parse.urlencode(data).encode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 请求对象定制</span></span><br><span class="line">    request = urllib.request.Request(url=url, data=data, headers=headers)</span><br><span class="line">    <span class="keyword">return</span> request</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_content</span>(<span class="params">request</span>):</span><br><span class="line">    response = urllib.request.urlopen(request)</span><br><span class="line">    content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> content</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">download</span>(<span class="params">content, page</span>):</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;kfc&#x27;</span> + <span class="built_in">str</span>(page) + <span class="string">&#x27;.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        fp.write(content)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    start_page = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&#x27;请输入起始的页码:&#x27;</span>))</span><br><span class="line">    end_page = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&#x27;请输入结束的页码:&#x27;</span>))</span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(start_page, end_page + <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 每一页都有请求对象的定制</span></span><br><span class="line">        request = create_request(page)</span><br><span class="line">        <span class="comment"># 获取响应数据</span></span><br><span class="line">        content = get_content(request)</span><br><span class="line">        <span class="comment"># 数据下载到本地</span></span><br><span class="line">        download(content, page)</span><br></pre></td></tr></table></figure>

<h3 id="异常"><a href="#异常" class="headerlink" title="异常"></a>异常</h3><p>URLError类是HttpError类的子类</p>
<p>导入的包<code>urllib.error</code>中的urllib.error.HttpError，urllib.error.URLError</p>
<p>http错误：http错误时针对浏览器无法连接到服务器而增加出来的错误提示。引导并告诉浏览器该页是哪里出了问题</p>
<p>通过urllib发送请求的时候，有可能会发送失败，这个时候如果想让代码更加健壮，可以通过try-except进行捕获异常，异常有两类URLError、HttpError</p>
<h3 id="cookie登录"><a href="#cookie登录" class="headerlink" title="cookie登录"></a>cookie登录</h3><p>适用的场景：在数据采集时候，需要绕过登录，进入到某个页面</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 爬取微博用户个人信息页面的数据</span></span><br><span class="line"><span class="comment"># 个人信息页面是utf-8,但是在爬取内容时依旧报错(编码错误),因为没有进入个人信息页面,而是进入了登录页面,登录页面不是utf-8</span></span><br><span class="line"><span class="comment"># 在请求头中的cookie中携带了登录信息</span></span><br><span class="line"><span class="comment"># referer起图片防盗链功能,判断路径是不是由上一个而来</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> urllib.parse</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://weibo.cn/.../info&#x27;</span></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="comment"># 浏览器中的request headers获取</span></span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 请求对象定制</span></span><br><span class="line">request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line"></span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;weibo.html&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    fp.write(content)</span><br></pre></td></tr></table></figure>

<h3 id="Handler处理器"><a href="#Handler处理器" class="headerlink" title="Handler处理器"></a>Handler处理器</h3><p>为什么需要handler处理器</p>
<p><code>urllib.request.Request(url=url, data=data, headers=headers)</code>可以定制请求头</p>
<p><code>urllib.request.urlopen(url)</code>不能定制请求头</p>
<p>Handler定制更高级的请求头（随着业务逻辑的复杂，请求对象的定制已经满足不了我们的需求（动态cookie和代理不能使用请求对象的定制））</p>
<h4 id="基本使用-1"><a href="#基本使用-1" class="headerlink" title="基本使用"></a>基本使用</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用handler来访问百度,获取网页原码</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line">url = <span class="string">&#x27;http://www.baidu.com&#x27;</span></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;user-agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取handler对象</span></span><br><span class="line">handler = urllib.request.HTTPHandler()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取opener对象</span></span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用open方法</span></span><br><span class="line">response = opener.<span class="built_in">open</span>(request)</span><br><span class="line"></span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(content)</span><br></pre></td></tr></table></figure>

<h4 id="代理服务器"><a href="#代理服务器" class="headerlink" title="代理服务器"></a>代理服务器</h4><p>代理的常用功能：</p>
<ul>
<li>突破自身的IP访问限制，访问国外站点</li>
<li>访问一些单位或团体内部资源</li>
<li>提高访问速度（通常代理服务器设置一个较大的硬盘缓冲区，当有外界信息通过时，同时也将其保存到缓冲区中，当其他用户再访问相同信息时，则直接由缓冲区中取出信息传给用户）</li>
<li>隐藏真实IP（上网者也可以通过这种方式隐藏自己的IP，免收攻击）</li>
</ul>
<p>代码配置代理：</p>
<ul>
<li>创建Request对象</li>
<li>创建ProxyHandler对象</li>
<li>用handler对象创建opener对象</li>
<li>使用opener.open函数发送请求</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用代理访问百度页面</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://www.baidu.com/s?wd=ip&#x27;</span></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;user-agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 请求对象定制</span></span><br><span class="line">request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line"></span><br><span class="line">poxies = &#123;</span><br><span class="line">    <span class="string">&#x27;http&#x27;</span>: <span class="string">&#x27;代理ip地址:端口号&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment"># 获取handler对象</span></span><br><span class="line">handler = urllib.request.ProxyHandler(proxies=poxies)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取opener对象</span></span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用open方法</span></span><br><span class="line">response = opener.<span class="built_in">open</span>(request)</span><br><span class="line"></span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;daili.html&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    fp.write(content)</span><br></pre></td></tr></table></figure>

<h4 id="代理池"><a href="#代理池" class="headerlink" title="代理池"></a>代理池</h4><p>在爬取过程中，如果ip被封则会出现很大困扰，一般我们使用代理池来解决</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 使用代理池中不同代理</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://www.baidu.com/s?wd=ip&#x27;</span></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;user-agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 请求对象定制</span></span><br><span class="line">request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义代理池</span></span><br><span class="line">proxies_pool = [</span><br><span class="line">    &#123;<span class="string">&#x27;http&#x27;</span>: <span class="string">&#x27;代理ip地址:端口号&#x27;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&#x27;http&#x27;</span>: <span class="string">&#x27;代理ip地址:端口号&#x27;</span>&#125;</span><br><span class="line">]</span><br><span class="line">proxies = random.choice(proxies_pool)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取handler对象</span></span><br><span class="line">handler = urllib.request.ProxyHandler(proxies=proxies)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取opener对象</span></span><br><span class="line">opener = urllib.request.build_opener(handler)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用open方法</span></span><br><span class="line">response = opener.<span class="built_in">open</span>(request)</span><br><span class="line"></span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;daili.html&#x27;</span>, <span class="string">&#x27;w&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    fp.write(content)</span><br></pre></td></tr></table></figure>

<h4 id="cookie库"><a href="#cookie库" class="headerlink" title="cookie库"></a>cookie库</h4><hr>
<h2 id="解析"><a href="#解析" class="headerlink" title="解析"></a>解析</h2><h3 id="xpath"><a href="#xpath" class="headerlink" title="xpath"></a>xpath</h3><p>xpath使用：需要安装xpath-helper插件（chrome浏览器扩展程序）重启浏览器后ctrl+shift+x出现小黑框</p>
<ul>
<li>安装lxml库：<code>pip install lxml</code></li>
<li>导入lxml.etree：<code>from lxml import etree</code></li>
<li>解析本地文件：<code>html_tree = etree.parse(&#39;xx.html&#39;)</code></li>
<li>解析服务器响应文件：<code>html_tree = etree.HTML(response.read().decode(&#39;utf-8&#39;))</code></li>
<li><code>html_tree.xpath(xpath路径)</code></li>
</ul>
<p>xpath基本语法：</p>
<ul>
<li>路径查询：<ul>
<li>&#x2F;&#x2F;：查询所有子孙节点，不考虑层级关系</li>
<li>&#x2F;：找直接子节点</li>
</ul>
</li>
<li>谓词查询：<ul>
<li><code>//div[@id]</code></li>
<li><code>//div[@id=&quot;maincontent&quot;]</code></li>
</ul>
</li>
<li>属性查询：<code>//@class</code></li>
<li>模糊查询：<ul>
<li>包含xx的：<code>//div[contains(@id,&#39;xx&#39;)]</code></li>
<li>以xx开头的：<code>//div[starts-with(@id,&#39;xx&#39;)]</code></li>
</ul>
</li>
<li>内容查询：<code>//div/h1/text()</code></li>
<li>逻辑运算：极少用到<ul>
<li>并且（and）：<code>//div[@id=&#39;head&#39; and @class=&#39;s_down&#39;]</code></li>
<li>或（|）<code>//title | //price</code></li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://www.baidu.com&#x27;</span></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="comment"># 解析网页源码</span></span><br><span class="line"><span class="comment"># 解析服务器响应文件</span></span><br><span class="line">tree = etree.HTML(content)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取想要的数据</span></span><br><span class="line"><span class="comment"># xpath的返回值是一个列表类型的数据</span></span><br><span class="line">result = tree.xpath(<span class="string">&#x27;//input[@id=&quot;su&quot;]/@value&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 爬取图片素材</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">creat_request</span>(<span class="params">page</span>):</span><br><span class="line">    <span class="keyword">if</span> page == <span class="number">1</span>:</span><br><span class="line">        url = <span class="string">&#x27;&#x27;</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        url = <span class="string">&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">    request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line">    <span class="keyword">return</span> request</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_content</span>(<span class="params">request</span>):</span><br><span class="line">    response = urllib.request.urlopen(request)</span><br><span class="line">    content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">    <span class="keyword">return</span> content</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">down_img</span>(<span class="params">content</span>):</span><br><span class="line">    <span class="comment"># 解析网页源码</span></span><br><span class="line">    <span class="comment"># 解析服务器响应文件</span></span><br><span class="line">    tree = etree.HTML(content)</span><br><span class="line">    <span class="comment"># 一般设计到图片的网页都涉及&#x27;懒&#x27;加载,需要使用懒加载之前数据获取</span></span><br><span class="line">    src_list = tree.xpath(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    name_list = tree.xpath(<span class="string">&#x27;&#x27;</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(name_list)):</span><br><span class="line">        name = name_list[i]</span><br><span class="line">        src = src_list[i]</span><br><span class="line">        url = <span class="string">&#x27;https:&#x27;</span> + src</span><br><span class="line">        <span class="comment"># 下载图片</span></span><br><span class="line">        urllib.request.urlretrieve(url=url, filename=<span class="string">&#x27;./img/&#x27;</span> + name + <span class="string">&#x27;.jpg&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    start_page = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&#x27;请输入起始页码:&#x27;</span>))</span><br><span class="line">    end_page = <span class="built_in">int</span>(<span class="built_in">input</span>(<span class="string">&#x27;请输入结束页码:&#x27;</span>))</span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> <span class="built_in">range</span>(start_page, end_page + <span class="number">1</span>):</span><br><span class="line">        <span class="comment"># 请求对象定制</span></span><br><span class="line">        request = creat_request(page)</span><br><span class="line">        content = get_content(request)</span><br><span class="line">        down_img(content)</span><br></pre></td></tr></table></figure>

<h3 id="jsonpath"><a href="#jsonpath" class="headerlink" title="jsonpath"></a>jsonpath</h3><p>jsonpath使用：</p>
<ul>
<li>pip安装：<code>pip install jsonpath</code></li>
<li>jsonpath使用：<ul>
<li><code>obj = json.load(open(&#39;json文件&#39;, &#39;r&#39;, encoding=&#39;utf-8&#39;))</code></li>
<li><code>ret = jsonpath.jsonpath(obj, &#39;jsonpath语法&#39;)</code></li>
</ul>
</li>
</ul>
<p>jsonpath只能解析本地文件</p>
<table>
<thead>
<tr>
<th align="center">XPath</th>
<th align="center">JSONPath</th>
<th>Description</th>
</tr>
</thead>
<tbody><tr>
<td align="center">&#x2F;</td>
<td align="center">$</td>
<td>表示根元素</td>
</tr>
<tr>
<td align="center">.</td>
<td align="center">@</td>
<td>当前元素</td>
</tr>
<tr>
<td align="center">&#x2F;</td>
<td align="center">. or []</td>
<td>子元素</td>
</tr>
<tr>
<td align="center">..</td>
<td align="center">n&#x2F;a</td>
<td>父元素</td>
</tr>
<tr>
<td align="center">&#x2F;&#x2F;</td>
<td align="center">..</td>
<td>递归下降</td>
</tr>
<tr>
<td align="center">*</td>
<td align="center">*</td>
<td>通配符，表示所有元素</td>
</tr>
<tr>
<td align="center">@</td>
<td align="center">n&#x2F;a</td>
<td>属性访问字符</td>
</tr>
<tr>
<td align="center">[]</td>
<td align="center">[]</td>
<td>子元素操作符</td>
</tr>
<tr>
<td align="center">|</td>
<td align="center">[,]</td>
<td>连接操作符，在XPath结果合并其他节点集合，JSONPath允许name或者数组索引</td>
</tr>
<tr>
<td align="center">n&#x2F;a</td>
<td align="center">[start: end:step]</td>
<td>数组分割操作</td>
</tr>
<tr>
<td align="center">[]</td>
<td align="center">?()</td>
<td>应用过滤表达式</td>
</tr>
<tr>
<td align="center">n&#x2F;a</td>
<td align="center">()</td>
<td>脚本表达式，使用在脚本引擎下面</td>
</tr>
<tr>
<td align="center">()</td>
<td align="center">n&#x2F;a</td>
<td>XPath分组</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 通过json解析淘票票网站首页的城市信息</span></span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> jsonpath</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://dianying.taobao.com/cityAction.json?activityId&amp;_ksTS=1675923454318_404&amp;jsoncallback=jsonp405&amp;action=cityAction&amp;n_s=new&amp;event_submit_doGetAllRegion=truehttps://dianying.taobao.com/cityAction.json?activityId&amp;_ksTS=1675923454318_404&amp;jsoncallback=jsonp405&amp;action=cityAction&amp;n_s=new&amp;event_submit_doGetAllRegion=true&#x27;</span></span><br><span class="line"></span><br><span class="line">headers = &#123;</span><br><span class="line">    </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">request = urllib.request.Request(url=url, headers=headers)</span><br><span class="line">response = urllib.request.urlopen(request)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"><span class="comment"># 处理成为标准的json文件</span></span><br><span class="line">content = content.split(<span class="string">&#x27;(&#x27;</span>)[<span class="number">1</span>].split(<span class="string">&#x27;)&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 写入本地文件</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;city.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    fp.write(content)</span><br><span class="line"></span><br><span class="line">obj = json.load(<span class="built_in">open</span>(<span class="string">&#x27;city.json&#x27;</span>,<span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line">city_list = jsonpath.jsonpath(obj, <span class="string">&#x27;$..regionName&#x27;</span>)</span><br><span class="line"><span class="built_in">print</span>(city_list)</span><br></pre></td></tr></table></figure>

<h3 id="BeautifulSoup"><a href="#BeautifulSoup" class="headerlink" title="BeautifulSoup"></a>BeautifulSoup</h3><p>简称为bs4，与lxml一样，是一个html的解析器，主要功能也是解析和提取数据</p>
<p>优点：接口设计人性化，使用方便</p>
<p>缺点：效率没有lxml的效率高</p>
<p>bs4的使用：</p>
<ul>
<li><p>安装：<code>pip install bs4</code></p>
</li>
<li><p>导包：<code>from bs4 import BeautifulSoup</code></p>
</li>
<li><p>创建对象：</p>
<ul>
<li><p>服务器响应的文件生成对象：<code>soup = BeautifulSoup(response.read().decode(&#39;utf-8&#39;), &#39;lxml&#39;)</code></p>
</li>
<li><p>本地文件生成对象：<code>soup = BeautifulSoup(open(&#39;xx.html&#39;), &#39;lxml&#39;)</code></p>
<p>注意默认打开文件的编码格式gbk，所以需要指定打开的编码格式</p>
</li>
</ul>
</li>
</ul>
<p>基本语法：</p>
<ul>
<li><p>根据标签名查找节点：<code>soup.a</code>只能找到第一个a，<code>soup.a.attrs</code>返回a中的属性值作为字典类型</p>
</li>
<li><p>函数：</p>
<ul>
<li><p>返回一个对象：<code>find()</code></p>
<p><code>find(&#39;a&#39;)</code>只能找到第一个a标签</p>
<p><code>find(&#39;a&#39;,title=&#39;xx&#39;)</code></p>
<p><code>find(&#39;a&#39;,class_=&#39;xx&#39;)</code></p>
</li>
<li><p>返回一个列表：<code>find_all()</code></p>
<p><code>find_all(&#39;a&#39;)</code>找到所有的a标签</p>
<p><code>find_all([&#39;a&#39;, &#39;span&#39;])</code>返回所有的a和span标签</p>
<p><code>find_all(&#39;a&#39;, limit=2)</code>只找前两个a</p>
</li>
<li><p>根据选择器得到节点对象（推荐）：<code>select</code></p>
<p><code>select(&#39;a&#39;)</code>找到所有的a标签</p>
<p><code>select(&#39;.a1&#39;)</code>找到class的值为a1的标签（类选择器）</p>
<p><code>select(&#39;#l1&#39;)</code>找到id的值l1的标签</p>
<p><code>select(&#39;li[id]&#39;)</code>找到li中有id的标签（属性选择器）</p>
<p>空格（后代）、大于（子代）、逗号（同级）（层级选择器）</p>
</li>
</ul>
</li>
</ul>
<p>节点信息：</p>
<ul>
<li>获取节点内容：适用于标签中嵌套标签的结构<code>obj.get_text()</code></li>
<li>节点的属性</li>
<li>获取节点属性：</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 爬取星巴克的菜单的商品图片及商品名称,并保存到本地</span></span><br><span class="line"><span class="keyword">from</span> urllib <span class="keyword">import</span> request</span><br><span class="line"><span class="keyword">from</span> bs4 <span class="keyword">import</span> BeautifulSoup</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://www.starbucks.com.cn/menu/&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">req = request.Request(url=url, headers=headers)</span><br><span class="line">response = request.urlopen(req)</span><br><span class="line">content = response.read().decode(<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line">soup = BeautifulSoup(content, <span class="string">&#x27;lxml&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># //ul[@class=&#x27;grid padded-3 product&#x27;]//strong/text()</span></span><br><span class="line">name_list = soup.select(<span class="string">&#x27;ul[class=&quot;grid padded-3 product&quot;] strong&#x27;</span>)</span><br><span class="line"><span class="keyword">for</span> name <span class="keyword">in</span> name_list:</span><br><span class="line">    <span class="built_in">print</span>(name.get_text())</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Selenium"><a href="#Selenium" class="headerlink" title="Selenium"></a>Selenium</h2><ul>
<li>Selenium是一个用于Web应用程序测试的工具</li>
<li>Selenium测试直接运行在浏览器中，就想真正的用户在操作一样</li>
<li>直接通过各种driver（FirfoxDriver、InternetExplorerDriver、OperaDriver、ChromeDriver）驱动真是浏览器完成测试</li>
<li>Selenium也是支持无界面浏览器操作的</li>
</ul>
<p>模拟浏览器功能，自动执行网页中的js代码，实现动态加载</p>
<h3 id="基本使用-2"><a href="#基本使用-2" class="headerlink" title="基本使用"></a>基本使用</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.chrome.service <span class="keyword">import</span> Service</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建浏览器操作对象</span></span><br><span class="line">service = Service(<span class="string">r&#x27;D:\Code\Python\pythonSpiderTest\chromedriver.exe&#x27;</span>)</span><br><span class="line">driver = webdriver.Chrome(service=service)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 访问网站</span></span><br><span class="line">url = <span class="string">&#x27;https://www.jd.com/&#x27;</span></span><br><span class="line">driver.get(url)</span><br><span class="line">content = driver.page_source</span><br><span class="line"><span class="built_in">print</span>(content)</span><br></pre></td></tr></table></figure>

<h3 id="元素定位"><a href="#元素定位" class="headerlink" title="元素定位"></a>元素定位</h3><p>自动化要做的就是模拟鼠标和键盘类操作这些元素，点击、输入等等。操作这些元素前首先要找到它们，WebDriver提供很多定位元素的方法</p>
<ul>
<li>利用ID查找：<code>find_element(By.ID, &quot;value&quot;)</code></li>
<li>利用类名查找：<code>find_element(By.CLASS_NAME, &quot;value&quot;)</code></li>
<li>利用name属性查找：<code>find_elements(By.NAME, &quot;value&quot;)</code></li>
<li>利用xpath查找：<code>find_element(By.XPATH, &quot;value&quot;)</code></li>
<li>利用标签名查找：<code>find_elements(By.TAG_NAME, &quot;value&quot;)</code></li>
<li>利用CSS选择器查找：<code>find_elements(By.CSS_SELETOR, &quot;value&quot;)</code></li>
</ul>
<h3 id="元素信息"><a href="#元素信息" class="headerlink" title="元素信息"></a>元素信息</h3><ul>
<li>获取元素属性：<code>get_attribute(&#39;class&#39;)</code></li>
<li>获取元素文本：<code>text</code></li>
<li>获取id：<code>id</code></li>
<li>获取标签名：<code>tag_name</code></li>
</ul>
<h3 id="交互"><a href="#交互" class="headerlink" title="交互"></a>交互</h3><ul>
<li><p>点击：<code>click()</code></p>
</li>
<li><p>输入：<code>send_keys()</code></p>
</li>
<li><p>后退操作：<code>driver.back()</code></p>
</li>
<li><p>前进操作：<code>driver.forword()</code></p>
</li>
<li><p>模拟JS滚动：</p>
<p><code>js=&#39;document.documentElement.scrollTop=100000&#39;</code></p>
<p><code>driver.execute_script(js)</code>执行JS代码</p>
</li>
<li><p>获取网页代码：<code>page_source</code></p>
</li>
<li><p>退出：<code>driver.quit()</code></p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 模拟浏览器进入baidu.com、搜索周杰伦、进入下一页、退回上一页、前进、退出</span></span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.chrome.service <span class="keyword">import</span> Service</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.common.by <span class="keyword">import</span> By</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建浏览器操作对象</span></span><br><span class="line">service = Service(executable_path=<span class="string">&#x27;D:\Code\Python\pythonSpiderTest\chromedriver.exe&#x27;</span>)</span><br><span class="line">driver = webdriver.Chrome(service=service)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 访问网站</span></span><br><span class="line">url = <span class="string">&#x27;https://www.baidu.com/&#x27;</span></span><br><span class="line">driver.get(url)</span><br><span class="line"><span class="comment"># 休眠</span></span><br><span class="line">time.sleep(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入框定位</span></span><br><span class="line"><span class="built_in">input</span> = driver.find_element(By.ID, <span class="string">&#x27;kw&#x27;</span>)</span><br><span class="line"><span class="comment"># 输入内容</span></span><br><span class="line"><span class="built_in">input</span>.send_keys(<span class="string">&#x27;周杰伦&#x27;</span>)</span><br><span class="line"><span class="comment"># 休眠</span></span><br><span class="line">time.sleep(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取百度一下按钮</span></span><br><span class="line">button = driver.find_element(By.ID, <span class="string">&#x27;su&#x27;</span>)</span><br><span class="line"><span class="comment"># 点击按钮</span></span><br><span class="line">button.click()</span><br><span class="line"><span class="comment"># 休眠</span></span><br><span class="line">time.sleep(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 向下滑动</span></span><br><span class="line">js = <span class="string">&#x27;document.documentElement.scrollTop = 100000&#x27;</span></span><br><span class="line">driver.execute_script(js)</span><br><span class="line"><span class="comment"># 休眠</span></span><br><span class="line">time.sleep(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 定位下一页</span></span><br><span class="line">button = driver.find_element(By.XPATH, <span class="string">&#x27;//a[@class=&quot;n&quot;]&#x27;</span>)</span><br><span class="line"><span class="comment"># 点击下一页</span></span><br><span class="line">button.click()</span><br><span class="line"><span class="comment"># 休眠</span></span><br><span class="line">time.sleep(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 退回上一页</span></span><br><span class="line">driver.back()</span><br><span class="line">time.sleep(<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 回到下一页</span></span><br><span class="line">driver.forward()</span><br><span class="line">time.sleep(<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 退出</span></span><br><span class="line">driver.quit()</span><br></pre></td></tr></table></figure>

<h3 id="Phantomjs"><a href="#Phantomjs" class="headerlink" title="Phantomjs"></a>Phantomjs</h3><p>目前不常使用</p>
<ul>
<li>一个无界面的浏览器</li>
<li>支持页面元素查找，js的执行等</li>
<li>由于不进行css和gui渲染，运行效率要比真实的浏览器快很多</li>
</ul>
<p>使用Phantomjs：</p>
<ul>
<li>获取PhantomJS.exe文件路径path</li>
<li><code>driver = webdriver.PhantomJS(path)</code></li>
<li><code>driver = get(url)</code></li>
<li>保存屏幕快照：<code>driver.save_screenshot(&#39;xx.png&#39;)</code></li>
</ul>
<h3 id="Chrome-handless"><a href="#Chrome-handless" class="headerlink" title="Chrome handless"></a>Chrome handless</h3><p>Chrome-handless模式，Google针对Chrome浏览器59版新增的一种模式，可以在不打开UI界面的情况下使用Chrome</p>
<p>系统要求：</p>
<ul>
<li><p>Linux&#x2F;Unix需要chrome&gt;&#x3D;50、Windows需要chrome&gt;&#x3D;60</p>
</li>
<li><p>Python&gt;&#x3D;3.6</p>
</li>
<li><p>Selenium&gt;&#x3D;3.4.*</p>
</li>
<li><p>ChromeDriver&gt;&#x3D;2.31</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 配置代码</span></span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.chrome.options <span class="keyword">import</span> Options</span><br><span class="line"></span><br><span class="line">chrome_options = Options()</span><br><span class="line">chrome_options.add_argument(<span class="string">&quot;--headless&quot;</span>)</span><br><span class="line">chrome_options.add_argument(<span class="string">&quot;disable-gpu&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自己的chrome浏览器路径</span></span><br><span class="line">path = <span class="string">r&#x27;C:\Program Files\Google\Chrome\Application\chrome.exe&#x27;</span></span><br><span class="line">chrome_options.binary_location = path</span><br><span class="line"></span><br><span class="line">driver = webdriver.Chrome(chrome_options=chrome_options)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 封装</span></span><br><span class="line"><span class="keyword">from</span> selenium <span class="keyword">import</span> webdriver</span><br><span class="line"><span class="keyword">from</span> selenium.webdriver.chrome.options <span class="keyword">import</span> Options</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">share_driver</span>():</span><br><span class="line">    chrome_options = Options()</span><br><span class="line">    chrome_options.add_argument(<span class="string">&quot;--headless&quot;</span>)</span><br><span class="line">    chrome_options.add_argument(<span class="string">&quot;disable-gpu&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 自己的chrome浏览器路径</span></span><br><span class="line">    path = <span class="string">r&#x27;C:\Program Files\Google\Chrome\Application\chrome.exe&#x27;</span></span><br><span class="line">    chrome_options.binary_location = path</span><br><span class="line">    </span><br><span class="line">    driver = webdriver.Chrome(chrome_options=chrome_options)</span><br><span class="line">    <span class="keyword">return</span> driver</span><br><span class="line"></span><br><span class="line">driver = share_driver()</span><br><span class="line">url = <span class="string">&#x27;xx.com&#x27;</span></span><br><span class="line">driver.get(url)</span><br></pre></td></tr></table></figure>

<hr>
<h2 id="Requests"><a href="#Requests" class="headerlink" title="Requests"></a>Requests</h2><p>Request是唯一一个非转基因的Python HTTP库</p>
<h3 id="基本使用-3"><a href="#基本使用-3" class="headerlink" title="基本使用"></a>基本使用</h3><p>官方文档：<a target="_blank" rel="noopener" href="http://cn.python_request.org/zh_CN/latest/">http://cn.python_request.org/zh_CN/latest/</a></p>
<p>快速上手：<a target="_blank" rel="noopener" href="http://cn.python_request.org/zh_CN/latest/user/quickstart.html">http://cn.python_request.org/zh_CN/latest/user/quickstart.html</a></p>
<p>安装：pip install requests</p>
<p>response的属性以及类型：<code>response = requests.get(url=url)</code></p>
<ul>
<li>类型：models.Request</li>
<li>response.text：获取网站源码</li>
<li>response.encoding：访问或定制编码方式</li>
<li>response.url：获取请求的url</li>
<li>response.content：响应的字节类型</li>
<li>response.status_code：响应的状态码</li>
<li>response.headers：响应的头信息</li>
</ul>
<h3 id="GET请求-1"><a href="#GET请求-1" class="headerlink" title="GET请求"></a>GET请求</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;http://www.baidu.com/s?&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;wd&#x27;</span>: <span class="string">&#x27;北京&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.get(url=url, params=data, headers=headers)</span><br><span class="line">content = response.text</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(content)</span><br></pre></td></tr></table></figure>

<p>参数使用params传递</p>
<p>参数无需urlencode编码</p>
<p>不需要进行请求对象定制</p>
<p>请求资源中的’?’可以加也可以不加</p>
<h3 id="POST请求-1"><a href="#POST请求-1" class="headerlink" title="POST请求"></a>POST请求</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line">url = <span class="string">&#x27;https://fanyi.baidu.com/sug&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;kw&#x27;</span>: <span class="string">&#x27;eye&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.post(url, headers=headers, data=data)</span><br><span class="line">content = response.text</span><br><span class="line"></span><br><span class="line">obj = json.loads(content, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(obj)</span><br></pre></td></tr></table></figure>

<p>post请求不需要编解码</p>
<p>post请求的参数是data</p>
<p>不需要进行请求对象定制</p>
<h3 id="代理"><a href="#代理" class="headerlink" title="代理"></a>代理</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line">url = <span class="string">&#x27;https://fanyi.baidu.com/s&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">data = &#123;</span><br><span class="line">    <span class="string">&#x27;wd&#x27;</span>: <span class="string">&#x27;ip&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">proxy = &#123;</span><br><span class="line">    <span class="string">&#x27;http&#x27;</span>: <span class="string">&#x27;xxx&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">response = requests.post(url, headers=headers, data=data, proxies=proxy)</span><br><span class="line"></span><br><span class="line">content = response.text</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;daili.html&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    fp.write(content)</span><br></pre></td></tr></table></figure>

<h3 id="cookie登录-1"><a href="#cookie登录-1" class="headerlink" title="cookie登录"></a>cookie登录</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> lxml <span class="keyword">import</span> etree</span><br><span class="line"></span><br><span class="line"><span class="comment"># 登录页面的url</span></span><br><span class="line">url = <span class="string">&#x27;https://so.gushiwen.cn/user/login.aspx?from=http://so.gushiwen.cn/user/collect.aspx&#x27;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&#x27;User-Agent&#x27;</span>: <span class="string">&#x27;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.116 Safari/537.36&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取页面的源码</span></span><br><span class="line">response = requests.post(url, headers=headers)</span><br><span class="line">content = response.text</span><br><span class="line"></span><br><span class="line"><span class="comment"># 解析页码源码,获取__viewstate,__viewstateGENERATOR</span></span><br><span class="line">html_tree = etree.HTML(content)</span><br><span class="line">__VIEWSTATE = html_tree.xpath(<span class="string">&#x27;//input[@name=&quot;__VIEWSTATE&quot;]/@value&#x27;</span>)</span><br><span class="line">__VIEWSTATEGENERATOR = html_tree.xpath(<span class="string">&#x27;//input[@name=&quot;__VIEWSTATEGENERATOR&quot;]/@value&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取验证码图片地址</span></span><br><span class="line">imgCode = html_tree.xpath(<span class="string">&#x27;//img[@id=&quot;imgCode&quot;]/@src&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line">code_url = <span class="string">&#x27;https://so.gushiwen.cn/&#x27;</span> + imgCode</span><br><span class="line"></span><br><span class="line"><span class="comment"># requests里面的session方法,通过session返回值就能使请求变成一个对象</span></span><br><span class="line">session = requests.session()</span><br><span class="line"><span class="comment"># 验证码的url的内容</span></span><br><span class="line">response_code = session.get(code_url)</span><br><span class="line"><span class="comment"># 图片下载需要使用功能二进制数据</span></span><br><span class="line">content_code = response_code.content</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;code.jpg&#x27;</span>, <span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    <span class="comment"># 写入图片</span></span><br><span class="line">    fp.write(content_code)</span><br><span class="line"></span><br><span class="line">code_name = <span class="built_in">input</span>(<span class="string">&#x27;验证码:&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 点击登录</span></span><br><span class="line">url_post = <span class="string">&#x27;https://so.gushiwen.cn/user/login.aspx?from=http%3a%2f%2fso.gushiwen.cn%2fuser%2fcollect.aspx&#x27;</span></span><br><span class="line">data_post = &#123;</span><br><span class="line">    <span class="string">&#x27;__VIEWSTATE&#x27;</span>: __VIEWSTATE[<span class="number">0</span>],</span><br><span class="line">    <span class="string">&#x27;__VIEWSTATEGENERATOR&#x27;</span>: __VIEWSTATEGENERATOR[<span class="number">0</span>],</span><br><span class="line">    <span class="string">&#x27;from&#x27;</span>: <span class="string">&#x27;http://so.gushiwen.cn/user/collect.aspx&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;email&#x27;</span>: <span class="string">&#x27;root@qq.com&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;pwd&#x27;</span>: <span class="string">&#x27;root&#x27;</span>,</span><br><span class="line">    <span class="string">&#x27;code&#x27;</span>: code_name,</span><br><span class="line">    <span class="string">&#x27;denglu&#x27;</span>: <span class="string">&#x27;登录&#x27;</span></span><br><span class="line">&#125;</span><br><span class="line">response_post = session.post(url=url_post, headers=headers, data=data_post)</span><br><span class="line">content_post = response_post.text</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;geshiwen.html&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> fp:</span><br><span class="line">    fp.write(content_post)</span><br></pre></td></tr></table></figure>

<p>通过模拟cookie登录古诗文网站，需要注意隐藏域和验证码问题</p>
<hr>
<h2 id="Scrapy"><a href="#Scrapy" class="headerlink" title="Scrapy"></a>Scrapy</h2><p>scrapy是一个为了爬取网站数据，提供结构性数据而编写的应用框架。可以应用在包括数据挖掘，信息处理或存储历史数据等一系列的程序过程</p>
<p>安装scrapy：pip install scrapy</p>
<h3 id="项目组成"><a href="#项目组成" class="headerlink" title="项目组成"></a>项目组成</h3><blockquote>
<p>spiders</p>
<blockquote>
<p>_<em>init.py</em>_</p>
<p>自定义爬虫文件.py（由我们自己创建，是实现爬虫核心功能的文件）</p>
</blockquote>
<p>_<em>init</em>_.py</p>
<p>item.py（定义数据结构的地方，是一个继承自scrapy.Item的类）</p>
<p>middlewares.py（中间件，代理）</p>
<p>pipeline.py（管道模式，里面只有一个类，用于处理下载数据的后需处理，默认是300优先级，值越小优先级越高（1-1000））</p>
<p>settings.py（配置文件）</p>
</blockquote>
<h4 id="scrapy项目的创建和运行"><a href="#scrapy项目的创建和运行" class="headerlink" title="scrapy项目的创建和运行"></a>scrapy项目的创建和运行</h4><ul>
<li><p>创建爬虫项目：<code>scrapy startproject 项目名</code></p>
<p>项目名不允许使用数字开头，也不允许出现中文</p>
</li>
<li><p>创建爬虫文件：</p>
<p>在spiders文件夹中创建爬虫文件：<code>cd 项目名\项目名\spiders</code></p>
<p>创建爬虫文件：<code>scrapy genspider 爬虫名字 爬取网页</code></p>
</li>
<li><p>运行爬虫代码：<code>scrapy crawl 爬虫名字</code></p>
</li>
</ul>
<p>获取响应字符串：<code>response.text</code></p>
<p>获取二进制数据：<code>response.body</code></p>
<p>直接使用xpath方法解析response内容：<code>response.xpath</code></p>
<p>提取seletor对象的data属性值：<code>response.extract()</code></p>
<p>提取seletor列表的第一个数据：<code>response.extract_first()</code></p>
<h4 id="scrapy架构组成"><a href="#scrapy架构组成" class="headerlink" title="scrapy架构组成"></a>scrapy架构组成</h4><ul>
<li>引擎：自动运行，无需关注，会自动组织所有的请求对象，分发给下载器</li>
<li>下载器：从引擎处获取到请求对象后，请求数据</li>
<li>spiders：Spiders类定义了如何爬取某个（或某些）网站。包括了爬取动作（例如：是否跟进链接）以及如何从网页的内容中提取结构化数据（爬取Items）</li>
<li>调度器：有自己的调度规则，无需关注</li>
<li>管道（Item pipelines）：最终处理数据的管道，会预留接口供我们处理数据（Item用于定义数据结构，piplines用于下载数据）</li>
</ul>
<p>当Item在spider中被收集之后，它将会被传递到Item pipeline，一些组件会按照一定的顺序执行对Item的处理每个item pipeline组件（有时称之为”Item pipeline”）是实现了简单方法的Python类。他们接收到Item并通过它执行一些行为，同时也决定此Item是否继续通过pipeline，或是被丢弃而不再进行处理</p>
<p>Item pipline的一些应用：</p>
<ul>
<li>清理HTML数据</li>
<li>验证爬虫的数据（检查Item包含某些字段）</li>
<li>查重（并丢弃）</li>
<li>将爬取结果保存到数据库中</li>
</ul>
<h4 id="scrapy工作原理"><a href="#scrapy工作原理" class="headerlink" title="scrapy工作原理"></a>scrapy工作原理</h4><div align="center">
    <img src="/2023/02/12/%E7%88%AC%E8%99%AB/scrapy工作原理.png" style="zoom:50%" alt="scrapy工作原理">
</div>


<div align="center">
    <img src="/2023/02/12/%E7%88%AC%E8%99%AB/scrapy工作原理en.png" style="zoom:50%" alt="scrapy工作原理en">
</div>
### scrapy shell

<p>Scrapy shell是Scrapy终端，是一个交互终端，供您在未启动spider的情况下尝试及调节您的代码。其本意是用来测试提取数据的代码，不过可以将其作为正常的python终端没在上面测试任何的python代码</p>
<p>该终端是用来测试XPath或CSS表达式，查看他们的工作方式及从爬取网页中提取的数据，在编写spider时，该终端提供了交互性测试您的表达式代码的功能，免去了每次修改后运行spider的麻烦</p>
<p>安装ipython：pip install ipython</p>
<p>如果您安装了IPython，Scrapy终端将使用IPython（代替标准的Python终端）。IPython终端与其他相比更为强大，提供智能的自动补全、高亮输出等特性</p>
<p>使用：</p>
<ul>
<li><code>scrapy shell www.xxx.com</code></li>
<li><code>scrapy shell http://www.xxx.com</code></li>
<li><code>scrapy shell &#39;http://www.xxx.com&#39;</code></li>
<li><code>scrapy shell &#39;www.xxx.com&#39;</code></li>
</ul>
<h3 id="yield"><a href="#yield" class="headerlink" title="yield"></a>yield</h3><p>带有yield的函数不再是一个普通函数，而是一个生成器generator，可用于迭代</p>
<p>yield是一个类似return的关键字，迭代一次遇到yield时就返回yield后面（右边）的值。重点是：下一次迭代时，从上一次迭代遇到的yield后面的代码（下一行）开始执行</p>
<p>简要理解：yield就是return返回一个值，并记住这个返回的位置，下次迭代就从这个位置后（下一行）开始</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 爬取当当网某类型图书的图片书名价格</span></span><br><span class="line"><span class="comment"># 自定义.py</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"><span class="keyword">from</span> dangdang.items <span class="keyword">import</span> DangdangItem</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DangSpider</span>(scrapy.Spider):</span><br><span class="line">    name = <span class="string">&quot;dang&quot;</span></span><br><span class="line">    allowed_domains = [<span class="string">&quot;category.dangdang.com&quot;</span>]</span><br><span class="line">    start_urls = [<span class="string">&quot;http://category.dangdang.com/cp01.01.02.00.00.00.html&quot;</span>]</span><br><span class="line"></span><br><span class="line">    base_url = <span class="string">&quot;http://category.dangdang.com/pg&quot;</span></span><br><span class="line">    page = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">parse</span>(<span class="params">self, response</span>):</span><br><span class="line">        <span class="comment"># src = //ul[@id=&#x27;component_59&#x27;]/li//img/@src</span></span><br><span class="line">        <span class="comment"># name = //ul[@id=&#x27;component_59&#x27;]/li//img/@alt</span></span><br><span class="line">        <span class="comment"># price = //ul[@id=&#x27;component_59&#x27;]/li//p[@class=&#x27;price&#x27;]/span[1]/text()</span></span><br><span class="line">        <span class="comment"># 所有的selector的对象 都可以再次调用xpath方法</span></span><br><span class="line">        li_list = response.xpath(<span class="string">&quot;//ul[@id=&#x27;component_59&#x27;]/li&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> li <span class="keyword">in</span> li_list:</span><br><span class="line">            <span class="comment"># 第一章图片和其他图片的标签属性不一样</span></span><br><span class="line">            <span class="comment"># 第一章图片的src可以使用 其他图片的地址是data-original</span></span><br><span class="line">            src = li.xpath(<span class="string">&quot;.//img/@data-original&quot;</span>).extract_first()</span><br><span class="line">            <span class="keyword">if</span> src:</span><br><span class="line">                src = src</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                src = li.xpath(<span class="string">&quot;.//img/@src&quot;</span>).extract_first()</span><br><span class="line">            name = li.xpath(<span class="string">&quot;.//img/@alt&quot;</span>).extract_first()</span><br><span class="line">            price = li.xpath(<span class="string">&quot;.//p[@class=&#x27;price&#x27;]/span[1]/text()&quot;</span>).extract_first()</span><br><span class="line">            book = DangdangItem(src=src, name=name, price=price)</span><br><span class="line">            <span class="comment"># 获取一个book就将book交给piplines</span></span><br><span class="line">            <span class="keyword">yield</span> book</span><br><span class="line"></span><br><span class="line">        <span class="comment"># http://category.dangdang.com/pg2-cp01.01.02.00.00.00.html</span></span><br><span class="line">        <span class="keyword">if</span> self.page &lt; <span class="number">100</span>:</span><br><span class="line">            self.page += <span class="number">1</span></span><br><span class="line">            url = self.base_url + <span class="built_in">str</span>(self.page) + <span class="string">&#x27;-cp01.01.02.00.00.00.html&#x27;</span></span><br><span class="line">            <span class="comment"># scrapy.Request就是Scrapy的get请求</span></span><br><span class="line">            <span class="comment"># url请求地址 callback是要执行的函数</span></span><br><span class="line">            <span class="keyword">yield</span> scrapy.Request(url=url, callback=self.parse)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># piplines.py</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define your item pipelines here</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Don&#x27;t forget to add your pipeline to the ITEM_PIPELINES setting</span></span><br><span class="line"><span class="comment"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># useful for handling different item types with a single interface</span></span><br><span class="line"><span class="keyword">from</span> itemadapter <span class="keyword">import</span> ItemAdapter</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果想使用管道,需要咋settings中开启</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DangdangPipeline</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">open_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.file = <span class="built_in">open</span>(<span class="string">&#x27;dangdang.json&#x27;</span>, <span class="string">&#x27;w&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># item就是yield后面的对象</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        self.file.write(<span class="built_in">str</span>(item) + <span class="string">&#x27;\n&#x27;</span>)</span><br><span class="line">        <span class="keyword">return</span> item</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">close_spider</span>(<span class="params">self, spider</span>):</span><br><span class="line">        self.file.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多条管道同时开启</span></span><br><span class="line"><span class="comment"># 定义管道类 在settings中开启管道</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DangdangPipeline2</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">process_item</span>(<span class="params">self, item, spider</span>):</span><br><span class="line">        url = <span class="string">&#x27;http:&#x27;</span> + item.get(<span class="string">&#x27;src&#x27;</span>)</span><br><span class="line">        filename = <span class="string">&#x27;./books/&#x27;</span> + item.get(<span class="string">&#x27;name&#x27;</span>) + <span class="string">&#x27;.jpg&#x27;</span></span><br><span class="line">        urllib.request.urlretrieve(url=url, filename=filename)</span><br><span class="line">        <span class="comment"># urllib.request.urlretrieve(url=url, filename=filename)</span></span><br><span class="line">        <span class="keyword">return</span> item</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># items.py</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define here the models for your scraped items</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># See documentation in:</span></span><br><span class="line"><span class="comment"># https://docs.scrapy.org/en/latest/topics/items.html</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> scrapy</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DangdangItem</span>(scrapy.Item):</span><br><span class="line">    <span class="comment"># define the fields for your item here like:</span></span><br><span class="line">    <span class="comment"># name = scrapy.Field()</span></span><br><span class="line">    <span class="comment"># 下载的数据内容</span></span><br><span class="line">    <span class="comment"># 图片</span></span><br><span class="line">    src = scrapy.Field()</span><br><span class="line">    <span class="comment"># 名字</span></span><br><span class="line">    name = scrapy.Field()</span><br><span class="line">    <span class="comment"># 价格</span></span><br><span class="line">    price = scrapy.Field()</span><br></pre></td></tr></table></figure>

<h3 id="CrawlSpider"><a href="#CrawlSpider" class="headerlink" title="CrawlSpider"></a>CrawlSpider</h3><p>继承自scrapy.Spider</p>
<p>CrawlSpider可以定义规则，在解析html内容的时候，可以根据链接规则提取出指定的链接，然后再向这些链接发送请求。如果有需要跟进链接需求，爬取网站之后需要提取链接再次进行爬取，可以使用CrawlSpider</p>
<p>提取链接：<code>scrapy.linkextractors.LinkExtractor</code></p>
<ul>
<li>正则表达式：<code>allow = ()</code></li>
<li>XPath：<code>restrict_xpaths = ()</code></li>
<li>选择器：<code>restrict_css = ()</code></li>
</ul>
<div align="center">
    <img src="/2023/02/12/%E7%88%AC%E8%99%AB/CrawlSpider.png" style="zoom:75%" alt="CrawlSpider">
</div>

<ul>
<li>提取链接：<code>link.extract_links(response)</code></li>
</ul>
<h3 id="日志信息与日志等级"><a href="#日志信息与日志等级" class="headerlink" title="日志信息与日志等级"></a>日志信息与日志等级</h3><p>日志级别：</p>
<ul>
<li>CRITICAL：严重错误</li>
<li>ERROR：一般错误</li>
<li>WARNING：警告</li>
<li>INFO：一般信息</li>
<li>DEBUG：调试信息</li>
</ul>
<p>默认的日志等级为DEBUG，只要出现DEBUG或DEBUG以上等级的日志全部打印</p>
<p>settings.py文件设置：</p>
<ul>
<li>LOG_FILE：将屏幕显示的信息全部记录在文件中，屏幕不在显示，文件后缀为.log</li>
<li>LOG_LEVEL：设置日志显示等级，就是显示哪些，不显示哪些</li>
</ul>
<h3 id="POST请求-2"><a href="#POST请求-2" class="headerlink" title="POST请求"></a>POST请求</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">TestSpider</span>(scripy.Spider):</span><br><span class="line">    name = <span class="string">&#x27;test&#x27;</span></span><br><span class="line">    allow_domains = [<span class="string">&#x27;xxx&#x27;</span>]</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">start_requests</span>(<span class="params">self</span>):</span><br><span class="line">        url = <span class="string">&#x27;xxx&#x27;</span></span><br><span class="line">        data = &#123;</span><br><span class="line">            <span class="string">&#x27;xx&#x27;</span>: <span class="string">&#x27;xx&#x27;</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">yield</span> scrapy.FormRequest(url=url, formdata=data, callback=self.parse_second)</span><br><span class="line">	<span class="keyword">def</span> <span class="title function_">parse_second</span>(<span class="params">self, response</span>):</span><br><span class="line">        content = response.text</span><br><span class="line">        obj = json.loads(content, encoding=<span class="string">&#x27;utf-8&#x27;</span>)</span><br></pre></td></tr></table></figure>

</article><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Notes/">Notes</a><a class="post-meta__tags" href="/tags/Language/">Language</a><a class="post-meta__tags" href="/tags/Spider/">Spider</a></div><div class="post_share"><div class="social-share" data-image="https://pic4.zhimg.com/v2-0b51b0f4b44bbdb55fbae70d14b299bf_180x120.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/02/17/Linux%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"><img class="prev-cover" src="https://tools.wingzero.tw/assets/upload/1639016963439_0.jpg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">Linux</div></div></a></div><div class="next-post pull-right"><a href="/2023/02/06/Flask/"><img class="next-cover" src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Flask_logo.svg/1200px-Flask_logo.svg.png" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Flask</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2023/03/14/C++/" title="C++"><img class="cover" src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/18/ISO_C%2B%2B_Logo.svg/1200px-ISO_C%2B%2B_Logo.svg.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-03-14</div><div class="title">C++</div></div></a></div><div><a href="/2023/02/06/Flask/" title="Flask"><img class="cover" src="https://upload.wikimedia.org/wikipedia/commons/thumb/3/3c/Flask_logo.svg/1200px-Flask_logo.svg.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-02-06</div><div class="title">Flask</div></div></a></div><div><a href="/2023/03/16/CMake/" title="CMake"><img class="cover" src="https://www.kitware.com/main/wp-content/uploads/2016/11/CMake-Logo-and-Text-e1540917038464.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-03-16</div><div class="title">CMake</div></div></a></div><div><a href="/2022/02/15/Golang/" title="Golang"><img class="cover" src="http://5b0988e595225.cdn.sohucs.com/images/20190127/98d2709000f649198967eb6e91d26a1a.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-02-15</div><div class="title">Golang</div></div></a></div><div><a href="/2023/05/11/C++%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" title="C++设计模式"><img class="cover" src="https://picx.zhimg.com/v2-f7e5e75bb391e9c74762d9dbee97f807_720w.jpg?source=172ae18b" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-11</div><div class="title">C++设计模式</div></div></a></div><div><a href="/2023/07/04/C++11/" title="C++11"><img class="cover" src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/18/ISO_C%2B%2B_Logo.svg/1200px-ISO_C%2B%2B_Logo.svg.png" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-07-04</div><div class="title">C++11</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/day.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">santidad DAY</div><div class="author-info__description">今日事，今日毕</div></div><div class="card-info-data is-center"><div class="card-info-data-item"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">17</div></a></div><div class="card-info-data-item"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">9</div></a></div><div class="card-info-data-item"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">1</div></a></div></div><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/santidadday" target="_blank" title="Github"><i class="fab fa-github"></i></a></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#html%E9%A1%B5%E9%9D%A2%E7%BB%93%E6%9E%84%E4%BB%8B%E7%BB%8D"><span class="toc-number">1.</span> <span class="toc-text">html页面结构介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%88%AC%E8%99%AB"><span class="toc-number">2.</span> <span class="toc-text">爬虫</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%88%AC%E8%99%AB%E6%A0%B8%E5%BF%83"><span class="toc-number">2.1.</span> <span class="toc-text">爬虫核心</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%88%AC%E8%99%AB%E7%94%A8%E9%80%94"><span class="toc-number">2.2.</span> <span class="toc-text">爬虫用途</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%88%AC%E8%99%AB%E5%88%86%E7%B1%BB"><span class="toc-number">2.3.</span> <span class="toc-text">爬虫分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%8D%E7%88%AC%E6%89%8B%E6%AE%B5"><span class="toc-number">2.4.</span> <span class="toc-text">反爬手段</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#urllib"><span class="toc-number">3.</span> <span class="toc-text">urllib</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8"><span class="toc-number">3.1.</span> <span class="toc-text">基本使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E7%B1%BB%E5%9E%8B%E5%92%8C%E6%96%B9%E6%B3%95"><span class="toc-number">3.2.</span> <span class="toc-text">常用类型和方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%8B%E8%BD%BD%E5%88%B0%E6%9C%AC%E5%9C%B0"><span class="toc-number">3.3.</span> <span class="toc-text">下载到本地</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%B7%E6%B1%82%E5%AF%B9%E8%B1%A1%E5%AE%9A%E5%88%B6"><span class="toc-number">3.4.</span> <span class="toc-text">请求对象定制</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E8%A7%A3%E7%A0%81"><span class="toc-number">3.5.</span> <span class="toc-text">编解码</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#GRT%E8%AF%B7%E6%B1%82%E6%96%B9%E5%BC%8F%EF%BC%9Aurllib-parse-quote"><span class="toc-number">3.5.1.</span> <span class="toc-text">GRT请求方式：urllib.parse.quote()</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GET%E8%AF%B7%E6%B1%82%E6%96%B9%E5%BC%8F%EF%BC%9Aurllib-parse-urlencode"><span class="toc-number">3.5.2.</span> <span class="toc-text">GET请求方式：urllib.parse.urlencode()</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#POST%E8%AF%B7%E6%B1%82%E6%96%B9%E5%BC%8F"><span class="toc-number">3.5.3.</span> <span class="toc-text">POST请求方式</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ajax"><span class="toc-number">3.6.</span> <span class="toc-text">ajax</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#GET%E8%AF%B7%E6%B1%82"><span class="toc-number">3.6.1.</span> <span class="toc-text">GET请求</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#POST%E8%AF%B7%E6%B1%82"><span class="toc-number">3.6.2.</span> <span class="toc-text">POST请求</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%82%E5%B8%B8"><span class="toc-number">3.7.</span> <span class="toc-text">异常</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cookie%E7%99%BB%E5%BD%95"><span class="toc-number">3.8.</span> <span class="toc-text">cookie登录</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Handler%E5%A4%84%E7%90%86%E5%99%A8"><span class="toc-number">3.9.</span> <span class="toc-text">Handler处理器</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8-1"><span class="toc-number">3.9.1.</span> <span class="toc-text">基本使用</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%90%86%E6%9C%8D%E5%8A%A1%E5%99%A8"><span class="toc-number">3.9.2.</span> <span class="toc-text">代理服务器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%A3%E7%90%86%E6%B1%A0"><span class="toc-number">3.9.3.</span> <span class="toc-text">代理池</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#cookie%E5%BA%93"><span class="toc-number">3.9.4.</span> <span class="toc-text">cookie库</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A7%A3%E6%9E%90"><span class="toc-number">4.</span> <span class="toc-text">解析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#xpath"><span class="toc-number">4.1.</span> <span class="toc-text">xpath</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#jsonpath"><span class="toc-number">4.2.</span> <span class="toc-text">jsonpath</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#BeautifulSoup"><span class="toc-number">4.3.</span> <span class="toc-text">BeautifulSoup</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Selenium"><span class="toc-number">5.</span> <span class="toc-text">Selenium</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8-2"><span class="toc-number">5.1.</span> <span class="toc-text">基本使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%83%E7%B4%A0%E5%AE%9A%E4%BD%8D"><span class="toc-number">5.2.</span> <span class="toc-text">元素定位</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%83%E7%B4%A0%E4%BF%A1%E6%81%AF"><span class="toc-number">5.3.</span> <span class="toc-text">元素信息</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BA%A4%E4%BA%92"><span class="toc-number">5.4.</span> <span class="toc-text">交互</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Phantomjs"><span class="toc-number">5.5.</span> <span class="toc-text">Phantomjs</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Chrome-handless"><span class="toc-number">5.6.</span> <span class="toc-text">Chrome handless</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Requests"><span class="toc-number">6.</span> <span class="toc-text">Requests</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8-3"><span class="toc-number">6.1.</span> <span class="toc-text">基本使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GET%E8%AF%B7%E6%B1%82-1"><span class="toc-number">6.2.</span> <span class="toc-text">GET请求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#POST%E8%AF%B7%E6%B1%82-1"><span class="toc-number">6.3.</span> <span class="toc-text">POST请求</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%A3%E7%90%86"><span class="toc-number">6.4.</span> <span class="toc-text">代理</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cookie%E7%99%BB%E5%BD%95-1"><span class="toc-number">6.5.</span> <span class="toc-text">cookie登录</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Scrapy"><span class="toc-number">7.</span> <span class="toc-text">Scrapy</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%A1%B9%E7%9B%AE%E7%BB%84%E6%88%90"><span class="toc-number">7.1.</span> <span class="toc-text">项目组成</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#scrapy%E9%A1%B9%E7%9B%AE%E7%9A%84%E5%88%9B%E5%BB%BA%E5%92%8C%E8%BF%90%E8%A1%8C"><span class="toc-number">7.1.1.</span> <span class="toc-text">scrapy项目的创建和运行</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#scrapy%E6%9E%B6%E6%9E%84%E7%BB%84%E6%88%90"><span class="toc-number">7.1.2.</span> <span class="toc-text">scrapy架构组成</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#scrapy%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86"><span class="toc-number">7.1.3.</span> <span class="toc-text">scrapy工作原理</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#yield"><span class="toc-number">7.2.</span> <span class="toc-text">yield</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#CrawlSpider"><span class="toc-number">7.3.</span> <span class="toc-text">CrawlSpider</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%A5%E5%BF%97%E4%BF%A1%E6%81%AF%E4%B8%8E%E6%97%A5%E5%BF%97%E7%AD%89%E7%BA%A7"><span class="toc-number">7.4.</span> <span class="toc-text">日志信息与日志等级</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#POST%E8%AF%B7%E6%B1%82-2"><span class="toc-number">7.5.</span> <span class="toc-text">POST请求</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>Recent Post</span></div><div class="aside-list"><div class="aside-list-item"><a class="thumbnail" href="/2023/12/01/Linux%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/" title="Linux网络编程"><img src="data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Linux网络编程"/></a><div class="content"><a class="title" href="/2023/12/01/Linux%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/" title="Linux网络编程">Linux网络编程</a><time datetime="2023-11-30T16:00:00.000Z" title="Created 2023-12-01 00:00:00">2023-12-01</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/11/03/ROS/" title="ROS基础"><img src="https://d2908q01vomqb2.cloudfront.net/ca3512f4dfa95a03169c5a670a4c91a19b3077b4/2018/11/26/ros-logo.jpg" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="ROS基础"/></a><div class="content"><a class="title" href="/2023/11/03/ROS/" title="ROS基础">ROS基础</a><time datetime="2023-11-02T16:00:00.000Z" title="Created 2023-11-03 00:00:00">2023-11-03</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/07/04/C++11/" title="C++11"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/1/18/ISO_C%2B%2B_Logo.svg/1200px-ISO_C%2B%2B_Logo.svg.png" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="C++11"/></a><div class="content"><a class="title" href="/2023/07/04/C++11/" title="C++11">C++11</a><time datetime="2023-07-03T16:00:00.000Z" title="Created 2023-07-04 00:00:00">2023-07-04</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/06/20/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B/" title="Linux系统编程"><img src="https://file.rymooc.com/SuperLargeCover/200303ae51194686d43e" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="Linux系统编程"/></a><div class="content"><a class="title" href="/2023/06/20/Linux%E7%B3%BB%E7%BB%9F%E7%BC%96%E7%A8%8B/" title="Linux系统编程">Linux系统编程</a><time datetime="2023-06-19T16:00:00.000Z" title="Created 2023-06-20 00:00:00">2023-06-20</time></div></div><div class="aside-list-item"><a class="thumbnail" href="/2023/05/11/C++%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" title="C++设计模式"><img src="https://picx.zhimg.com/v2-f7e5e75bb391e9c74762d9dbee97f807_720w.jpg?source=172ae18b" onerror="this.onerror=null;this.src='/img/404.jpg'" alt="C++设计模式"/></a><div class="content"><a class="title" href="/2023/05/11/C++%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/" title="C++设计模式">C++设计模式</a><time datetime="2023-05-10T16:00:00.000Z" title="Created 2023-05-11 00:00:00">2023-05-11</time></div></div></div></div></div></div></main><footer id="footer"><div id="footer-wrap"><div class="copyright">&copy;2021 - 2025 By santidad DAY</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">welcome  to  my  <a  target="_blank" rel="noopener" href="https://butterfly.js.org/">blog</a>!</div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.js"></script><div class="js-pjax"></div><script defer="defer" id="ribbon" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/canvas-ribbon.min.js" size="150" alpha="0.6" zIndex="-1" mobile="true" data-click="true"></script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-heart.min.js" async="async" mobile="true"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>